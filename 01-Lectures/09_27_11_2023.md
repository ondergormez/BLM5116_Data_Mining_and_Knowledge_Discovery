# 9. Week - 27 November 2023 Monday

## 9. Hafta - 25 Kasım 2024 Pazartesi - Vize 1

* Normalizasyon sorusu vardı. Bir nümerik feature (maaş bilgisi diye hatırlıyorum) un min-max scaler, min-max scaler [-1, 1] ve standard scaler ile normalize edilmesi istenmişti. 
  * Min-Max scaler [0, 1] arası
  * Min-Max scaler [-1, 1] arası
  * Standard scaler (Z-Score) kullanılarak normalize edilmesi
* Naive Bayes sorusu vardı. 
  * Formülü yazıp her bir ihtimali hesaplamamız istenmişti.
* Karar ağacı (Decision Tree) çizdirme sorusu vardı.
  * 3 tane feature verilmişti. 2 sinin gaini aynıydı.
  * Root Kökü bulmak için her bir feature un entropy sini hesaplamamız istenmişti.
  * Gain ı en büyük olan root kök.
  * Ordan itibaren aşağı doğru ağaç çıkarıldı.
* Bir tane KNN sorusu vardı. k = 1 için 1000 örneği olan bir veri setinde sonuçlar elde edilmiş.
  * 600 traininig, 400 test olarak ayrılmış.
  * 600 üzerinde train yapılmış.
  * 400 üzerinde test yapılmış. Testlerde %8 error rate elde edilmiş.
  * 400 ün üzerine 100 tane train verisi yanlışlıkla override edildiği testler sonrasında gözlemlenmiş.
  * Bu durumda gerçek error rate ne olur? Soruluyordu.
  * 100 tane yanlış train verisi olduğu için 100 tane yanlış test verisi olmuş oluyor.
  * 400 * (8/100) = 32 yanlış test verisi olmuş oluyor.
  * 400 - 100 = 300 test verisi üzerinde 32 yanlış oluyor.
  * 32 / 300 = 0.10666666666666667 yani %10.67 error rate oluyor.

# Bilgilendirme
* 18 Aralık 2023 2. vize sınavı yapılıyor olacak.
* Sunumlar 1 hafta yapılıyor olacak. 
* Bir günde 7-8 adet sunum yapılabiliyor. Dolayısyla herkes sunamayacak.
* Rapor teslim tarihini 18 Aralık tan 25 Aralık a çekildi.
  * Derste sunum yapmak isteyenler gelip yapacak.
  * Derste yapmak istemeyenler video çekip ders duvarına yükleyecekler.
  * Ayrıca makale ile ilgili bir rapor için online yıldız da ödev açıyor olacak.
  * Sunum dışında bir rapor da hazırlanacak. 18 sayfa bir makale için 4-5 sayfa rapor yeterlidir.
  * Makale bir veriseti paylaşılıyorsa mutlaka inceleyin.
  * Mutlaka verisetine dokunun, bir şeyler yapmayı deneyin
  * Bir takım kolay feature lar varsa benzer toollarla sizde sonuçları oluşturmaya çalışın.
  * Veriseti yoksa kendiniz benzer bir veriseti bulabilirsiniz.

# Clustering Methods: Hierarchical Clustering

Not: 07. sunumdan devam ediyoruz.
* A hierarchical clustering works by grouping objects.

* [Hierarchical Cluster Analysis [Simply explained]](https://www.youtube.com/watch?v=8QCBl-xdeZI) videosunda bulunan bilgiler faydalı olabilir.
* Burada bir [Online Statistics Calculator](https://datatab.net/statistics-calculator/cluster) refer ediliyor. Bununla Hierarchical Clustering denenerek girilen örnekler için sonuçlar elde edilebilir.
* Daha fazla detay gerekirse videonun son dakikalarını izle.

| Samples | x_axis | y_axis |
|:-------:|:------:|:------:|
|    x1   |    0   |    2   |
|    x2   |    0   |    0   |
|    x3   |   1.5  |    0   |
|    x4   |    5   |    0   |
|    x5   |    5   |    2   |

| Sample ID | I1 | I2 | I3 |
|:---------:|:--:|:--:|:--:|
|     X1    | 25 | 16 | 59 |
|     X2    | 72 | 43 | 21 |
|     X3    | 34 | 58 | 16 |
|     X4    | 56 | 36 | 68 |
|     X5    | 48 | 72 | 31 |
|     X6    | 81 | 49 | 83 |
|     X7    | 63 | 48 | 24 |

## Divisive
* Top down strategy
* Bir tane büyük cluster ve bunu parçalayarak alt clusterlara bölüyoruz.
* Bölme işlemi kolay olmadığı için pratikte çok kullanılmayan bir yöntemdir.

## Dendrogram
A dendrogram is a diagram representing a tree. This diagrammatic representation is frequently used in different contexts: 
* In hierarchical clustering, it illustrates the arrangement of the clusters produced by the corresponding analyses. [wikipedia](https://en.wikipedia.org/wiki/Dendrogram)
![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c5/UPGMA_Dendrogram_Hierarchical.svg/800px-UPGMA_Dendrogram_Hierarchical.svg.png)

## Agglomerative
* Elinizdeki her sample kendi başına bir üye (atomic cluster)
* Bu atomic cluster lar birleştirilerek yeni kümeler oluşturuluyor.
* Tüm kümeler (cluster) birleştirilerek tek bir cluster oluncaya kadar devam ediliyor.

* Minimum Euclidean Distance: 2 farklı cluster da bulunan 2 objenin arasında bulunan en kısa mesafedir.

Aşağıda sık kullanılan mesafe ölçüm metodları verilmiştir.
* Minimum Distance (dmin): 2 küme içerisinde bulunan objelerden arasında en kısa mesafesi bulunan ikisidir.
* Max Distance (dmax): 2 küme içerisinde bulunan objelerden arasında en uzun mesafesi bulunan ikisidir.
* Mean Distance (dmean): 2 küme içerisinde blunan objelerin ortalama mesafelerini vermektedir.
* Avarage Distance: Mean distance ile aynı sonucu döndürmüyor.

### Linkage Methods

#### Single Linkage
**Cluster lar arası mesafe** olarak dmin kullandıysam single oluyor.
* Dezavantajı gürültüye karşı (aykırı değerler) duyarlıdır.

TODO: Vize 2 de çıkacak olan örnek.

#### Complete Linkage
**Cluster lar arası mesafe** olarak dmax kullandıysam complete oluyor.
* Dezavantajı gürültüye karşı (aykırı değerler) duyarlıdır.
* TODO: Vize 2 de belki çıkabilir olan örnek.

#### Average Linkage
davarage kullandıysam average linkage oluyor.

### Variance Methods

#### Ward's Method
Kümelerin birbirlerine olan uzaklıklarını alıyor.

### Centroid Methods

# What is a Good Clustering?

* Kümeler arasındaki benzerliğin düşük olması gerekmektedir.
* Küme içi benzerliğin yüksek olması gerekmektedir.
